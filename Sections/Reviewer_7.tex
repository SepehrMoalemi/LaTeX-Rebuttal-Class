\section*{Reviewer 7}\label{sec:reviewer7}
\renewcommand{\theequation}{R7.\arabic{equation}}
\setcounter{equation}{0}
% -------------------------------------------------------------------------------------------- %
\begin{rebuttal}
    % Comment
    {%
        This is a well written paper.
    }%
    % Response
    {%
    }%
\end{rebuttal}
% -------------------------------------------------------------------------------------------- %
\begin{rebuttal}
    % Comment
    {%
        In the introduction, contribution (1) seems to be inappropriate, as earlier the paper says that something similar is done in [3-6].
    }%
    % Response
    {%
    }%
\end{rebuttal}
% -------------------------------------------------------------------------------------------- %
\begin{rebuttal}
    % Comment
    {%
        If I understand correctly, the main technique is the reformulation by adding $D$ terms in Figure 3. At a high level, this seems like a small modification, and even though it may be a new way to study gradient descent, it is not clear what are the benefits of this view points. To be more clear, I think the paper does not prove stability/convergence under any new stepsize rules, that were not known before by other methods. This is discussed in Sec IV-A if I am right.
    }%
    % Response
    {%
    }%
\end{rebuttal}
% -------------------------------------------------------------------------------------------- %
\begin{rebuttal}
    % Comment
    {%
        I found the discussion of gain scheduling approaches the most interesting ones, however I have some concern. The numerical result only seems to offer a small benefit in Table I, as opposed to other known methods. As a result, this can be used as a criticism, that the benefits of the approach are very minimal. It would be great if the authors can clarify whether this is always the case.
    }%
    % Response
    {%
    }%
\end{rebuttal}
% -------------------------------------------------------------------------------------------- %