\section*{Reviewer 8}\label{sec:reviewer8}
\renewcommand{\theequation}{R8.\arabic{equation}}
\setcounter{equation}{0}
% -------------------------------------------------------------------------------------------- %
\begin{rebuttal}[resolved]
    % Comment
    {%
        It is well known that the gradient descent dynamics for unconstrained (convex) optimization problems are passive, even more generally in distributed optimization and multi-agent equilibrium problems, and even subject to convex constraints.
    }%
    % Response
    {%
        We have looked at the work by DÃ¶rfler~\cite{dorfler_energy}, Pavel~\cite{pavel_passivity_ct}, Na Li~\cite{li_passivity}, and Notarstefano~\cite{Notarstefano}. A \textbf{discrete-time} passivity-based analysis of the GD method is not considered in these works. More specifically, the passivity-based analysis of the GD method in~\cite{pavel_passivity_ct,li_passivity} is a \textbf{continues-time} analysis. In fact, as discussed in Section~III of the original manuscript, the GD controller \(\bm{\mathcal{G}}\), as presented in~\cite{lessard_recht_iqc}, is strictly proper and, therefore, \textbf{not} passive. Consequently, the first contribution of this work is to provide a passive interpretation of the GD method in discrete-time by introducing a feedthrough term via a loop transformation. To remove any ambiguity, we now explicitly state \emph{discrete-time} in the title, abstract, introduction, and the closing remarks of the revised manuscript.
    }%
\end{rebuttal}
% -------------------------------------------------------------------------------------------- %
\begin{rebuttal}[resolved]
    % Comment
    {%
        It is also known that asymptotic convergence to an optimal solution holds if the step size are taken small enough. There are several works in the literature that quantify how small the step size should be and how fast a time-varying step size should vanish. The conclusions made in Section V, e.g.\ summarized in Table I,  are therefore to be expected. From this perspective, both the ``Control interpretation'' in Section II  and the convergence results (Theorem 3) in Section III are - mathematically speaking - already known. Furthermore, there are already several papers that attach a system theoretic interpretation to the gradient descent algorithm in convex optimization. Consequently, this reviewer is not clear neither on the novel theoretical contribution of the paper, nor on the more practical, technical contributions. In turn, this reviewer cannot see the implications of the paper for the field of operations research and/or for the field of systems and control theory.
    }%
    % Response
    {%
        Thank you for the comment. Indeed, there are many existing results related to step size, convergence, and a systems theoretic interpretation of gradient descent. What distinguishes our work from the existing literature in~\cite{ugrinovskii,alex_petersen,hu_lessard,lessard_dissipativity,lessard_recht_iqc,seiler_iqc,simpson} is that the discrete-time passive-systems interpretation, facilitated by a loop transformation, enables 
        \begin{itemize}
            \item{%
                the proof of input-output stability of the GD method for \(\alpha = 2/L\) via the weak version of the passivity theorem, and
            }%
            \item{%
                leveraging of other passivity-based results, such as gain-scheduling.
            }%
        \end{itemize}
        In fact, in Section~IV-B of the revised manuscript, we leverage the discrete-time passivity interpretation to enable the use of a gain-scheduling method that, ultimately, leads to a new kind of variable step size GD method.
    }%
\end{rebuttal}
% -------------------------------------------------------------------------------------------- %
\begin{rebuttal}[stuck]
    % Comment
    {%
    From a technical perspective, given that potential non-convexity of the $f$ function, it is unclear what is $\mbf{x}^{\ast}$ in Section IV\@. Furthermore, one should assume that the sub-differential of $f$ is bounded, a mathematical detail that is omitted in the paper.
    }%
    % Response
    {%
        Thank you for your comment. This paper considers a continuously differentiable functions \(f \in \mathcal{S}_{m, L}\) with \(m, L \in \mathbb{R}_{>0}\), such that \(m \leq L\), and its unique global minimizer \(\mbf{x}^\ast\). This class of functions has sector-bounded gradients and contains non-convex functions as-well. Consequently, for \(f \in \mathcal{S}_{m, L}\), the sub-differential of \(f\) is necessarily bounded since \(f\) is continuously differentiable and its gradient is sector-bounded.
    }%
\end{rebuttal}
% -------------------------------------------------------------------------------------------- %